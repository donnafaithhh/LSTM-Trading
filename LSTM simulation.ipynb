{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c873ec37-8b38-45bd-a5f3-916547e51961",
   "metadata": {},
   "source": [
    "This was created by Donna Faith Go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582cb46-40a5-43e0-bd65-bd35053ac28d",
   "metadata": {},
   "source": [
    "# LSTM Trading\n",
    "In this notebook, I will learn how to implement an LSTM model using pyTorch on a randomly generated portfolio from the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31cc858-54d9-4066-9f66-8ab739806616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:08.307938Z",
     "iopub.status.busy": "2025-12-24T15:13:08.307938Z",
     "iopub.status.idle": "2025-12-24T15:13:23.590775Z",
     "shell.execute_reply": "2025-12-24T15:13:23.590775Z",
     "shell.execute_reply.started": "2025-12-24T15:13:08.307938Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# data gathering\n",
    "import yfinance as yf\n",
    "\n",
    "# LSTM neural network \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# comparing metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# for portfolio generation\n",
    "from scipy.optimize import minimize\n",
    "import portfolio\n",
    "\n",
    "# for lstm train test split\n",
    "from typing import Tuple\n",
    "\n",
    "# standardizing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ensuring pep8\n",
    "%load_ext pycodestyle_magic\n",
    "\n",
    "# ignore future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e7ed41-5889-4098-ac20-10c715d33ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:23.592781Z",
     "iopub.status.busy": "2025-12-24T15:13:23.591780Z",
     "iopub.status.idle": "2025-12-24T15:13:23.596178Z",
     "shell.execute_reply": "2025-12-24T15:13:23.595017Z",
     "shell.execute_reply.started": "2025-12-24T15:13:23.592781Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b7a5a-0b1a-4dfe-86dc-80f4b74b968a",
   "metadata": {},
   "source": [
    "## LSTM from Geeks for Geeks\n",
    "I first learned how to implement an LSTM neural network from [the Geeks for Geeks website](https://www.geeksforgeeks.org/deep-learning/long-short-term-memory-networks-using-pytorch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38fe1034-9781-4ea7-92d4-9ab2c93fa7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:23.597215Z",
     "iopub.status.busy": "2025-12-24T15:13:23.597215Z",
     "iopub.status.idle": "2025-12-24T15:13:23.605161Z",
     "shell.execute_reply": "2025-12-24T15:13:23.604068Z",
     "shell.execute_reply.started": "2025-12-24T15:13:23.597215Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for comparison\n",
    "cols = [\n",
    "    'dataset used', 'MAPE', 'MAE', 'MSE', 'R2'\n",
    "]\n",
    "results_df = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddd2d9-1026-46e9-85b7-190ff7d461ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "### Geeks for Geeks Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f34145-f98b-4155-81e1-0667c2b57c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:23.606162Z",
     "iopub.status.busy": "2025-12-24T15:13:23.606162Z",
     "iopub.status.idle": "2025-12-24T15:13:23.674510Z",
     "shell.execute_reply": "2025-12-24T15:13:23.674510Z",
     "shell.execute_reply.started": "2025-12-24T15:13:23.606162Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "t = np.linspace(0, 100, 1000)\n",
    "data = np.sin(t)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\\\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "trainX = torch.tensor(X[:, :, None], dtype=torch.float32)\n",
    "trainY = torch.tensor(y[:, None], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad46c16-de7d-4e69-b92a-6ab228177be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:23.675540Z",
     "iopub.status.busy": "2025-12-24T15:13:23.675540Z",
     "iopub.status.idle": "2025-12-24T15:13:23.681305Z",
     "shell.execute_reply": "2025-12-24T15:13:23.681305Z",
     "shell.execute_reply.started": "2025-12-24T15:13:23.675540Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(\n",
    "                0), self.hidden_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(\n",
    "                0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step\n",
    "        return out, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf26f0c-2ece-45de-befc-5e9d2938be24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:23.682560Z",
     "iopub.status.busy": "2025-12-24T15:13:23.682560Z",
     "iopub.status.idle": "2025-12-24T15:13:25.480656Z",
     "shell.execute_reply": "2025-12-24T15:13:25.480656Z",
     "shell.execute_reply.started": "2025-12-24T15:13:23.682560Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=1, hidden_dim=100, layer_dim=1, output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edffdc3c-bf74-4332-acb3-088ae2edb0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:25.481883Z",
     "iopub.status.busy": "2025-12-24T15:13:25.481883Z",
     "iopub.status.idle": "2025-12-24T15:13:28.547783Z",
     "shell.execute_reply": "2025-12-24T15:13:28.547783Z",
     "shell.execute_reply.started": "2025-12-24T15:13:25.481883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2866\n",
      "Epoch [20/100], Loss: 0.0900\n",
      "Epoch [30/100], Loss: 0.0379\n",
      "Epoch [40/100], Loss: 0.0184\n",
      "Epoch [50/100], Loss: 0.0027\n",
      "Epoch [60/100], Loss: 0.0004\n",
      "Epoch [70/100], Loss: 0.0008\n",
      "Epoch [80/100], Loss: 0.0003\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "h0, c0 = None, None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    h0, c0 = h0.detach(), c0.detach()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ffc4f1-8fdc-48ce-b638-493b27484da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:28.548787Z",
     "iopub.status.busy": "2025-12-24T15:13:28.548787Z",
     "iopub.status.idle": "2025-12-24T15:13:28.807209Z",
     "shell.execute_reply": "2025-12-24T15:13:28.806202Z",
     "shell.execute_reply.started": "2025-12-24T15:13:28.548787Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predicted, _, _ = model(trainX, h0, c0)\n",
    "\n",
    "original = data[seq_length:]\n",
    "time_steps = np.arange(seq_length, len(data))\n",
    "\n",
    "predicted[::30] += 0.2\n",
    "predicted[::70] -= 0.2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_steps, original, label='Original Data')\n",
    "plt.plot(time_steps, predicted.detach().numpy(),\n",
    "         label='Predicted Data', linestyle='--')\n",
    "plt.title('LSTM Model Predictions vs. Original Data')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('figures/LSTM Model Predictions vs. Original Data.png')\n",
    "plt.close()\n",
    "\n",
    "# getting metrics\n",
    "mape_val = mean_absolute_percentage_error(original, predicted.detach().numpy())\n",
    "mae_val = mean_absolute_error(original, predicted.detach().numpy())\n",
    "mse_val = mean_squared_error(original, predicted.detach().numpy())\n",
    "r2_val = r2_score(original, predicted.detach().numpy())\n",
    "\n",
    "# saving to pandas dataframe\n",
    "new_row = {\n",
    "    'dataset used': 'sin data', \n",
    "    'MAPE': mape_val, \n",
    "    'MAE': mae_val, \n",
    "    'MSE': mse_val, \n",
    "    'R2': r2_val\n",
    "}\n",
    "results_df.loc[len(results_df)] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf95ce9-1616-4b2a-bf19-eed4aba917f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "### Other Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37746b81-e1db-49aa-8685-caea619a83fb",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src='figures/Stochastic vs Stationary data.png' />\n",
    "</div>\n",
    "\n",
    "In this section, we want to try looking at the performance of the LSTM model on stationary data. \n",
    "Stationary data is when there is no unit root present in the dataset.\n",
    "Usually, data is turned stationary to make patterns in the dataset more stable and predictable for forecasting models. \n",
    "Non-stationary data with trends or seasonality usually confuses these models, leading to unreliable forecasts.\n",
    "One of the common ways to turn stochastic data to stationary data is to difference it, apply log transformations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591345d-f2e4-4ab5-a422-21b4008c6526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "#### Stationary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8070ab7-4d2d-4da6-b10c-9e3eccdabca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:28.807209Z",
     "iopub.status.busy": "2025-12-24T15:13:28.807209Z",
     "iopub.status.idle": "2025-12-24T15:13:28.964155Z",
     "shell.execute_reply": "2025-12-24T15:13:28.963148Z",
     "shell.execute_reply.started": "2025-12-24T15:13:28.807209Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate 1000 data points\n",
    "np.random.seed(42)\n",
    "num_steps = 1000\n",
    "steps = np.random.choice([-1, 1], size=num_steps)\n",
    "random_walk = np.cumsum(steps)\n",
    "start_value = 50\n",
    "random_walk_with_start = start_value + np.concatenate([[0], random_walk[:-1]])\n",
    "\n",
    "# make it stationary\n",
    "stationary_data = np.diff(random_walk_with_start)\n",
    "\n",
    "# plot the data\n",
    "fig , ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "ax[0].plot(random_walk_with_start, label='Stochastic Data')\n",
    "ax[0].set_ylabel('Position')\n",
    "ax[0].set_title('Stochastic Data')\n",
    "\n",
    "ax[1].plot(stationary_data, label='Stationary Data')\n",
    "ax[1].set_ylabel('Difference')\n",
    "ax[1]. set_title('Stationary Data')\n",
    "fig.supylabel('Position')\n",
    "fig.supxlabel('Step Number')\n",
    "fig.suptitle('Stochastic vs Stationary data')\n",
    "plt.savefig('figures/Stochastic vs Stationary data.png')\n",
    "plt.close()\n",
    "# generate x and y\n",
    "seq_length = 20\n",
    "X, y = create_sequences(stationary_data, seq_length)\n",
    "trainX = torch.tensor(X[:, :, None], dtype=torch.float32)\n",
    "trainY = torch.tensor(y[:, None], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08323e15-00c1-4d30-ba8e-3459ea4ce2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:13:28.964155Z",
     "iopub.status.busy": "2025-12-24T15:13:28.964155Z",
     "iopub.status.idle": "2025-12-24T15:14:21.989586Z",
     "shell.execute_reply": "2025-12-24T15:14:21.989586Z",
     "shell.execute_reply.started": "2025-12-24T15:13:28.964155Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.9932\n",
      "Epoch [200/1000], Loss: 0.9245\n",
      "Epoch [300/1000], Loss: 0.2104\n",
      "Epoch [400/1000], Loss: 0.0034\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0001\n",
      "Epoch [900/1000], Loss: 0.0002\n",
      "Epoch [1000/1000], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "h0, c0 = None, None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    h0, c0 = h0.detach(), c0.detach()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cf5973-7c11-40ef-a3c2-511f08e58256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:14:21.991034Z",
     "iopub.status.busy": "2025-12-24T15:14:21.991034Z",
     "iopub.status.idle": "2025-12-24T15:14:22.250228Z",
     "shell.execute_reply": "2025-12-24T15:14:22.250228Z",
     "shell.execute_reply.started": "2025-12-24T15:14:21.991034Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predicted, _, _ = model(trainX, h0, c0)\n",
    "\n",
    "original = stationary_data[seq_length:]\n",
    "time_steps = np.arange(seq_length, len(stationary_data))\n",
    "\n",
    "predicted[::30] += 0.2\n",
    "predicted[::70] -= 0.2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_steps, original, label='Original Data')\n",
    "plt.plot(time_steps, predicted.detach().numpy(),\n",
    "         label='Predicted Data', linestyle='--')\n",
    "plt.title('LSTM Model Predictions vs. Original Stationary Data')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('figures/LSTM Model Predictions vs. Original Stationary Data.png')\n",
    "plt.close()\n",
    "\n",
    "# getting metrics\n",
    "mape_val = mean_absolute_percentage_error(original, predicted.detach().numpy())\n",
    "mae_val = mean_absolute_error(original, predicted.detach().numpy())\n",
    "mse_val = mean_squared_error(original, predicted.detach().numpy())\n",
    "r2_val = r2_score(original, predicted.detach().numpy())\n",
    "\n",
    "# saving to pandas dataframe\n",
    "new_row = {\n",
    "    'dataset used': 'stationary data', \n",
    "    'MAPE': mape_val, \n",
    "    'MAE': mae_val, \n",
    "    'MSE': mse_val, \n",
    "    'R2': r2_val\n",
    "}\n",
    "results_df.loc[len(results_df)] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8209dff-d274-4b17-84d3-2932b3d74ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "#### Stochastic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d5a2c6-cf31-4eef-819b-856c5e465405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:14:22.251235Z",
     "iopub.status.busy": "2025-12-24T15:14:22.251235Z",
     "iopub.status.idle": "2025-12-24T15:14:22.256064Z",
     "shell.execute_reply": "2025-12-24T15:14:22.256064Z",
     "shell.execute_reply.started": "2025-12-24T15:14:22.251235Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate x and y\n",
    "seq_length = 20\n",
    "X, y = create_sequences(random_walk_with_start, seq_length)\n",
    "trainX = torch.tensor(X[:, :, None], dtype=torch.float32)\n",
    "trainY = torch.tensor(y[:, None], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb9a2970-e2e7-471c-b0d9-7eef7e5a5cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:14:22.257293Z",
     "iopub.status.busy": "2025-12-24T15:14:22.257293Z",
     "iopub.status.idle": "2025-12-24T15:15:17.010389Z",
     "shell.execute_reply": "2025-12-24T15:15:17.009380Z",
     "shell.execute_reply.started": "2025-12-24T15:14:22.257293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 77.3714\n",
      "Epoch [200/1000], Loss: 24.6150\n",
      "Epoch [300/1000], Loss: 13.7973\n",
      "Epoch [400/1000], Loss: 6.0461\n",
      "Epoch [500/1000], Loss: 3.5963\n",
      "Epoch [600/1000], Loss: 2.2803\n",
      "Epoch [700/1000], Loss: 1.7464\n",
      "Epoch [800/1000], Loss: 1.5498\n",
      "Epoch [900/1000], Loss: 1.4895\n",
      "Epoch [1000/1000], Loss: 1.2694\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "h0, c0 = None, None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    h0, c0 = h0.detach(), c0.detach()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e48ab47-c47b-4ddb-9583-2f2fd9e8d6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:17.010389Z",
     "iopub.status.busy": "2025-12-24T15:15:17.010389Z",
     "iopub.status.idle": "2025-12-24T15:15:17.149510Z",
     "shell.execute_reply": "2025-12-24T15:15:17.149510Z",
     "shell.execute_reply.started": "2025-12-24T15:15:17.010389Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predicted, _, _ = model(trainX, h0, c0)\n",
    "\n",
    "original = random_walk_with_start[seq_length:]\n",
    "time_steps = np.arange(seq_length, len(random_walk_with_start))\n",
    "\n",
    "predicted[::30] += 0.2\n",
    "predicted[::70] -= 0.2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_steps, original, label='Original Data')\n",
    "plt.plot(time_steps, predicted.detach().numpy(),\n",
    "         label='Predicted Data', linestyle='--')\n",
    "plt.title('LSTM Model Predictions vs. Original Stochastic Data')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig('figures/LSTM Model Predictions vs. Original Stochastic Data.png')\n",
    "plt.close()\n",
    "\n",
    "# getting metrics\n",
    "mape_val = mean_absolute_percentage_error(original, predicted.detach().numpy())\n",
    "mae_val = mean_absolute_error(original, predicted.detach().numpy())\n",
    "mse_val = mean_squared_error(original, predicted.detach().numpy())\n",
    "r2_val = r2_score(original, predicted.detach().numpy())\n",
    "\n",
    "# saving to pandas dataframe\n",
    "new_row = {\n",
    "    'dataset used': 'stochastic data', \n",
    "    'MAPE': mape_val, \n",
    "    'MAE': mae_val, \n",
    "    'MSE': mse_val, \n",
    "    'R2': r2_val\n",
    "}\n",
    "results_df.loc[len(results_df)] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da5317-4df4-4791-ba28-87ab301950c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "### Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b01bbb56-bc68-4350-afa8-018286ce8e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:17.150951Z",
     "iopub.status.busy": "2025-12-24T15:15:17.150951Z",
     "iopub.status.idle": "2025-12-24T15:15:17.166838Z",
     "shell.execute_reply": "2025-12-24T15:15:17.165832Z",
     "shell.execute_reply.started": "2025-12-24T15:15:17.150951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset used</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin data</td>\n",
       "      <td>0.130210</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.996812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stationary data</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.998481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stochastic data</td>\n",
       "      <td>0.016232</td>\n",
       "      <td>1.003409</td>\n",
       "      <td>1.270952</td>\n",
       "      <td>0.990606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset used      MAPE       MAE       MSE        R2\n",
       "0         sin data  0.130210  0.013631  0.001608  0.996812\n",
       "1  stationary data  0.011643  0.011643  0.001518  0.998481\n",
       "2  stochastic data  0.016232  1.003409  1.270952  0.990606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9adc8-40ef-4ccf-93cb-639ae81c4324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T00:22:44.225785Z",
     "iopub.status.busy": "2025-12-20T00:22:44.225785Z",
     "iopub.status.idle": "2025-12-20T00:22:44.237862Z",
     "shell.execute_reply": "2025-12-20T00:22:44.236087Z",
     "shell.execute_reply.started": "2025-12-20T00:22:44.225785Z"
    }
   },
   "source": [
    "Based on the table above, it is clear that LSTM works well on either simple datasets, stochastic data, and stationary data. \n",
    "It must be noted that, although it performs better on stationary data, it is not necessarily the better choice because its performance improvement is very minimal.\n",
    "Hence, adding this preprocessing step would not be good or important for the dataset. \n",
    "\n",
    "I want to add that I also tried doing the LSTM with different epochs, and I saw that with more epochs, the LSTM NN works better (I mean obviously! ðŸ™„).\n",
    "However, simpler datasets especially those with seasonality like the sin/cosine wave that was shown in the Geeks for Geeks example can benefit from an LSTM that has less epochs because it is less comuptationally expensive and delivers results that are already satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e8c66-9607-42c8-8e2e-7c98f3d88ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2a56e-210e-46bd-9aa4-d55b2c038936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "For this portion, we are randomly selecting 100 stocks from the S&P 500 to create our portfolio.\n",
    "The size of 100 was chosen because we want a diverse portfolio.\n",
    "Furthermore, we also scraped the GSPC stock price because it is the ticker symbol for the S&P 500.\n",
    "Usually, this is used as a benchmark for the overall US stock market to determine how well a portfolio or a trading strategy is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ad1bf7-8724-4a90-8695-ea6cb004605d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:17.166838Z",
     "iopub.status.busy": "2025-12-24T15:15:17.166838Z",
     "iopub.status.idle": "2025-12-24T15:15:18.072193Z",
     "shell.execute_reply": "2025-12-24T15:15:18.071184Z",
     "shell.execute_reply.started": "2025-12-24T15:15:17.166838Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting the stock tickers\n",
    "headers = {\n",
    "    'User-Agent': (\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "        '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    )\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
    "    headers=headers\n",
    ")\n",
    "response.raise_for_status()\n",
    "tables = pd.read_html(response.text)\n",
    "\n",
    "if len(tables) > 0:\n",
    "    stocks_df = tables[0]\n",
    "\n",
    "# randomly selecting 30 stocks\n",
    "random_stocks = stocks_df['Symbol'].sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd36cf75-e582-48d1-8339-b9bedd35facf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:18.072193Z",
     "iopub.status.busy": "2025-12-24T15:15:18.072193Z",
     "iopub.status.idle": "2025-12-24T15:15:18.080011Z",
     "shell.execute_reply": "2025-12-24T15:15:18.080011Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.072193Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting closing prices for the 30 stocks with batching\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "def download_stocks_in_batches(tickers, batch_size=5, delay=1):\n",
    "    \"\"\"\n",
    "    Download stock data in batches to avoid rate limiting\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers[i:i + batch_size]\n",
    "        print(f\"Downloading batch {i//batch_size + 1}: {batch}\")\n",
    "        \n",
    "        try:\n",
    "            # Download the batch\n",
    "            batch_data = yf.download(\n",
    "                batch,\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            # Extract closing prices for this batch\n",
    "            if not batch_data.empty and 'Close' in batch_data.columns:\n",
    "                closes = batch_data['Close']\n",
    "                if isinstance(closes, pd.Series):\n",
    "                    all_data[batch[0]] = closes\n",
    "                else:\n",
    "                    for ticker in closes.columns:\n",
    "                        all_data[ticker] = closes[ticker]\n",
    "                print(f\"Successfully downloaded {len(batch)} stocks\")\n",
    "            else:\n",
    "                print(f\"No data returned for batch: {batch}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading batch {batch}: {e}\")\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        if i + batch_size < len(tickers):\n",
    "            print(f\"Waiting {delay} seconds before next batch...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.DataFrame(all_data)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# # Download in batches of 5 stocks with 1-second delay\n",
    "# closing_df = download_stocks_in_batches(\n",
    "#     random_stocks.tolist(), \n",
    "#     batch_size=5, \n",
    "#     delay=15\n",
    "# )\n",
    "\n",
    "# if not closing_df.empty:\n",
    "#     closing_df.to_pickle('data/closing prices.pkl')\n",
    "\n",
    "# closing_df.head(5)\n",
    "\n",
    "# # Download in batches of 5 stocks with 1-second delay\n",
    "# closing_df = download_stocks_in_batches(\n",
    "#     ['^GSPC'], \n",
    "#     batch_size=5, \n",
    "#     delay=15\n",
    "# )\n",
    "\n",
    "# if not closing_df.empty:\n",
    "#     closing_df.to_pickle('data/gspc prices.pkl')\n",
    "\n",
    "# closing_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2856d2e-ca86-4bf6-a6f0-c3b0b669a330",
   "metadata": {},
   "source": [
    "Note: The code above takes around 5 minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f9fd19-fafc-4a8e-a53d-ded1fc0a3fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:18.081059Z",
     "iopub.status.busy": "2025-12-24T15:15:18.081059Z",
     "iopub.status.idle": "2025-12-24T15:15:18.113907Z",
     "shell.execute_reply": "2025-12-24T15:15:18.113907Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.081059Z"
    }
   },
   "outputs": [],
   "source": [
    "# opening pkl file\n",
    "filename = r'data/closing prices.pkl'\n",
    "with open(filename, 'rb') as f: \n",
    "    closing_df = pickle.load(f)\n",
    "    closing_df.index = pd.to_datetime(closing_df.index)\n",
    "\n",
    "# getting s&p 500 data\n",
    "filename = r'data/gspc prices.pkl'\n",
    "with open(filename, 'rb') as f: \n",
    "    gspc_df = pickle.load(f)\n",
    "    gspc_df.index = pd.to_datetime(gspc_df.index)\n",
    "gspc_df.rename(columns={'^GSPC': 'GSPC'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d2a56-9dc3-4601-8e95-1e080e0042e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Portfolio Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff72c67-e964-4de1-a33d-02f729ec8d4a",
   "metadata": {},
   "source": [
    "Usually, assigning weights yearly for the firms in your portfolio would make more sense.\n",
    "Patterns in firms change as they have to account for new external factors every year such as shifting market conditions, competition from other firms, and more. \n",
    "By changing the weights annually, we are able to incorporate recent financial performance and adjust for any structural changes in the companies.\n",
    "\n",
    "To make things simple, I will use the Markowitz Portfolio Optimization model to assign weights to the different stocks.\n",
    "Here, the best portfolios were chosen based on their Sharpe ratio.\n",
    "For context, the Sharpe Ratio measures an investment's risk-adjusted return. \n",
    "It indicates how much extra return you get for the extra volatility (risk) you take on compared to a risk-free asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d23638f-8a6c-4096-a0c8-8042c7bc609b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:15:18.114950Z",
     "iopub.status.busy": "2025-12-24T15:15:18.114950Z",
     "iopub.status.idle": "2025-12-24T15:15:18.121027Z",
     "shell.execute_reply": "2025-12-24T15:15:18.120019Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.114950Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'year', 'expected return', 'volatility', 'sharpe ratio'\n",
    "]\n",
    "results_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "# # generate portfolios\n",
    "# years = ['2022', '2023', '2024']\n",
    "# for year in years:\n",
    "#     p = portfolio.generate_portfolio(\n",
    "#         closing_df.loc[year],\n",
    "#         year\n",
    "#     )\n",
    "#     weights, results, stocks = p.get_portfolio()\n",
    "#     ef_portfolio = pd.Series(weights, index=stocks)\n",
    "#     ef_portfolio.to_pickle(\n",
    "#         f'portfolios/{year} ef portfolio.pkl'\n",
    "#     )\n",
    "#     results_df.loc[len(results_df)] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1673fb-3ff0-4173-8789-12ac4c63069b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## EDA I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a530f-8461-45d7-bc4e-e686cfc1d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get historical data\n",
    "historical_data = closing_df.copy()\n",
    "\n",
    "# check nulls\n",
    "if any(historical_data.isnull().sum() > 1):\n",
    "    print('There are still some nulls for historical data.')\n",
    "else:\n",
    "    print('There are no more nulls for historical data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb4a7d-91b3-4537-92a8-a75871013ced",
   "metadata": {},
   "source": [
    "Since there are some columns with nulls, we are going to have to drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8168128-a86b-4627-9989-f1e95bfa51fa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.500586Z",
     "iopub.status.idle": "2025-12-24T15:15:18.501587Z",
     "shell.execute_reply": "2025-12-24T15:15:18.501587Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.501587Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly selecting stocks\n",
    "np.random.seed(42)\n",
    "random_stocks = np.random.choice(historical_data.columns, 5)\n",
    "\n",
    "# plotting the closing prices of random stocks\n",
    "plt.figure(figsize=(15, 5))\n",
    "for stock in random_stocks:\n",
    "    plt.plot(historical_data[stock], label=stock)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Prices')\n",
    "plt.ylim(0, 300)\n",
    "plt.title('Closing Prices of Randomly Chosen Stocks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacefec2-afcc-49bb-a54e-8ba2da71694f",
   "metadata": {},
   "source": [
    "Based on the figure above, we can see that there are significant differences in the closing stock prices. \n",
    "We'll have to turn the data into log returns or standardize the data in order to make meaningful comparisons.\n",
    "This transformation will also make the time series more stationary which makes it easier for machine learning models to forecast price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d24e62-1be6-46cf-8db3-27ca5d687b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4405e49-795d-4da7-98db-29754ab8bc99",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.502586Z",
     "iopub.status.idle": "2025-12-24T15:15:18.503586Z",
     "shell.execute_reply": "2025-12-24T15:15:18.502586Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.502586Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping stocks with null columns\n",
    "historical_data = historical_data.dropna(axis=1, how='any')\n",
    "\n",
    "# get log returns\n",
    "log_returns = historical_data.pct_change().dropna(how='all')\n",
    "print(f'There are a total of {len(log_returns.columns)} remaining after feature engineering.')\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(historical_data.values)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=historical_data.columns)\n",
    "\n",
    "# check nulls for log returns\n",
    "if any(log_returns.isnull().sum() > 1):\n",
    "    print('There are still some nulls for log returns.')\n",
    "else:\n",
    "    print('There are no more nulls for log returns.')\n",
    "\n",
    "# check nulls for standardized closing prices\n",
    "if any(standardized_df.isnull().sum() > 1):\n",
    "    print('There are still some nulls for standardized data.')\n",
    "else:\n",
    "    print('There are no more nulls for standardized data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77c47d-f930-4bf6-be15-4fc5001a503b",
   "metadata": {},
   "source": [
    "When training the model, we want to make sure that it is not handling any null data because this might affect how well it can forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becffacb-a058-4c7d-a946-749c004c0718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## EDA II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9b266-2cb9-475a-a8e5-796ae2328b7c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.504588Z",
     "iopub.status.idle": "2025-12-24T15:15:18.504588Z",
     "shell.execute_reply": "2025-12-24T15:15:18.504588Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.504588Z"
    }
   },
   "outputs": [],
   "source": [
    "# get correlations\n",
    "correl_matrix = log_returns.corr()\n",
    "order = correl_matrix.mean().sort_values(ascending=False).index\n",
    "correl_matrix_sorted = correl_matrix.loc[order, order]\n",
    "\n",
    "# plotting\n",
    "plt.figure()\n",
    "sns.heatmap(\n",
    "    correl_matrix_sorted, cmap='YlGn', fmt=\".2f\", vmin=0, vmax=1\n",
    ")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5241124-730d-430c-94fc-82374cc6cea3",
   "metadata": {},
   "source": [
    "The heatmap of the correlation matrix shows that a lot of the stocks, based on their log returns, are closer to weak to negligible correlation.\n",
    "This entails that we have a diverse portfolio which is ideal for mitigating unsystematic risk within our trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236d35a-328b-4743-aea9-9a356ada0ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## LSTM NN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8906298-fdd7-4fe8-893e-2079a1240fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "First, we have to define the functions to be used by the machine learning mode. \n",
    "The functions for the splitting the time series with a lookback window is from Professor Ethan Robert A. Casin's lectures for our Machine Learning II course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e9821-33fc-43fc-8ca0-866262dd9e4c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.507586Z",
     "iopub.status.idle": "2025-12-24T15:15:18.507586Z",
     "shell.execute_reply": "2025-12-24T15:15:18.507586Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.507586Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split with lookback\n",
    "def train_test_split(data: pd.Series, lookback: int = 12, test_size: int = 12) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Create a train-test split on your pd.Series data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data (pd.Series): The univariate time series provided\n",
    "    lookback (int): The lookback periods to consider\n",
    "    test_size (int): The length of the test set\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple (pd.Series): The (train, test) pandas series generated\n",
    "    \"\"\"\n",
    "    \n",
    "    split = data.shape[0] - test_size\n",
    "    return (\n",
    "        data.iloc[:split],\n",
    "        data.iloc[split - lookback: ]\n",
    "    )\n",
    "\n",
    "# split data into features and target\n",
    "def create_xy(series: pd.Series, lookback: int = 12, horizon: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create the X and y arrays from a pd.Series object. The `lookback`\n",
    "    determines the number of features we would include in X. While the\n",
    "    `horizon` paramter informs us how many points we'll need to forecast.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    series (pd.Series): the (n, 1) time series to be sliced into X and Y\n",
    "    lookback (int): the lookback window to consider (default=12)\n",
    "    horizon (int): the number of points to forecast for each row in X (default=1)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    (X, y) (Tuple): a tuple of ndarrays\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    series_size = series.shape[0]\n",
    "    for i in range(series_size):\n",
    "        \n",
    "        # break loop if series is less than the required time horizon\n",
    "        if series.iloc[(i + lookback): (i + lookback + horizon)].shape[0] < horizon:\n",
    "            break\n",
    "        x.append(\n",
    "            series.iloc[i: (i + lookback)]\n",
    "        )\n",
    "        y.append(\n",
    "            series.iloc[(i + lookback): (i + lookback + horizon)]\n",
    "        )\n",
    "    x = np.dstack(x)\n",
    "    y = np.dstack(y)\n",
    "\n",
    "    # Reshape x to (samples, features, lookback)\n",
    "    x = np.swapaxes(x, 0, 2)\n",
    "    # Then flatten (samples, features * lookback)\n",
    "    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])\n",
    "\n",
    "    # Reshape y tp (samples, horizon)\n",
    "    y = np.swapaxes(y, 0, 2)\n",
    "    y = y[:, :, 0]\n",
    "    \n",
    "    return x, np.array(y).flatten()\n",
    "\n",
    "# evaluating model performance\n",
    "def evaluate_model(model, x, y):\n",
    "    outputs = model(x)\n",
    "    y_pred = outputs.detach().numpy()\n",
    "    y_true = y.detach().numpy()\n",
    "    \n",
    "    mape_val = mean_absolute_percentage_error(y_true,y_pred)\n",
    "    mae_val = mean_absolute_error(y_true, y_pred)\n",
    "    mse_val = mean_squared_error(y_true,y_pred)\n",
    "    r2_val = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return mape_val, mae_val, mse_val, r2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b8939-1d28-4a28-848d-391544b6ed66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9d144-8831-43e4-83bf-98bd578b97a1",
   "metadata": {},
   "source": [
    "The LSTM model follows the usual architecture, but for this implementation, we will only train one model for all 100 stocks in our portfolio.\n",
    "\n",
    "**Insert a more detailed explanation of the model architecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2f21e-6d8e-42bd-ad58-ad622cd2fa52",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.509587Z",
     "iopub.status.idle": "2025-12-24T15:15:18.509587Z",
     "shell.execute_reply": "2025-12-24T15:15:18.509587Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.509587Z"
    }
   },
   "outputs": [],
   "source": [
    "# LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size, \n",
    "            hidden_size = hidden_size, \n",
    "            num_layers = num_layers, \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # weight_decay=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557067f2-874f-4897-9277-31d291ffe3d9",
   "metadata": {},
   "source": [
    "ðŸ’¡ Something new I learned today:\n",
    "- The `weight_decay` parameter in the Adam optimizer is something that is different from batch normalization. Here, it penalizes large weights in the neural network to ensure that there is no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26900340-6c0c-4cc6-b3a9-d37913363063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc051c78-aab0-4256-a8b3-005f76ea712d",
   "metadata": {},
   "source": [
    "Note: The code below takes more than 5 hours to run on a CPU. \n",
    "I highly recommend that you ask someone with a GPU to run this code or run it overnight when you don't have to use your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d4667-fb0b-4962-9fa4-3a4b15162c7f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.510588Z",
     "iopub.status.idle": "2025-12-24T15:15:18.511587Z",
     "shell.execute_reply": "2025-12-24T15:15:18.510588Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.510588Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining parameters for training\n",
    "train_ratio = 0.8\n",
    "epochs = 750\n",
    "lookback = 252 # last year of trading\n",
    "\n",
    "# create dataframe for model performance\n",
    "cols = [\n",
    "    'MAPE', 'MAE', 'MSE', 'R2'\n",
    "]\n",
    "idx = log_returns.columns.to_list()\n",
    "results_df = pd.DataFrame(columns=cols, index=idx)\n",
    "\n",
    "# training and storing predictions\n",
    "all_predictions = {}\n",
    "for firm in standardized_df.columns:\n",
    "    # features and target\n",
    "    X, y = create_xy(standardized_df[firm], lookback=lookback, horizon=1) # 20 days because 20 trading days\n",
    "\n",
    "    # train and validation sets\n",
    "    split_index = int(len(X) * train_ratio)\n",
    "    x_train, x_val = X[:split_index], X[split_index:]\n",
    "    y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "    # datasets into tensors\n",
    "    x_train_tensors = torch.from_numpy(x_train).float()\n",
    "    x_val_tensors = torch.from_numpy(x_val).float()\n",
    "    y_train_tensors = torch.from_numpy(y_train).float()\n",
    "    y_val_tensors = torch.from_numpy(y_val).float()\n",
    "\n",
    "    # missing input_dim dimension\n",
    "    x_train_tensors = x_train_tensors.unsqueeze(-1)\n",
    "    x_val_tensors = x_val_tensors.unsqueeze(-1)\n",
    "\n",
    "    # model features\n",
    "    input_size = 1\n",
    "    hidden_size = lookback # i think they should be the same\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    \n",
    "    # instantiate model\n",
    "    model = LSTMModel(\n",
    "        input_size, hidden_size, num_layers, output_size\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # weight_decay=0.1\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train_tensors)\n",
    "        loss = criterion(outputs, y_train_tensors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}'\n",
    "            )\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val_tensors)\n",
    "        val_loss = criterion(val_outputs, y_val_tensors)\n",
    "        print(\n",
    "            f'Validation Loss: {val_loss.item(): .2f}'\n",
    "        )\n",
    "\n",
    "    # get daily predictions\n",
    "    daily_preds = []\n",
    "    val_series = standardized_df[firm][-len(x_val_tensors)-lookback:] \n",
    "    for i in range(len(x_val_tensors)):\n",
    "        x_seq = val_series.iloc[i:i+lookback].values\n",
    "        x_tensor = torch.tensor(x_seq).float().unsqueeze(0).unsqueeze(-1)\n",
    "        x_tensor = x_tensor.to(next(model.parameters()).device)\n",
    "        pred = model(x_tensor)\n",
    "        daily_preds.append(pred.item())\n",
    "        \n",
    "    all_predictions[firm] = pd.Series(\n",
    "        daily_preds,\n",
    "        index=standardized_df.index[-len(x_val_tensors):]\n",
    "    )\n",
    "\n",
    "    # evaluate model performance\n",
    "    mape_val, mae_val, mse_val, r2_val = evaluate_model(model, x_val_tensors, y_val_tensors)\n",
    "    results_df.loc[firm] = [mape_val, mae_val, mse_val, r2_val]\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), f'models/{firm} model.pth')\n",
    "    print(f'Done for {firm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18d4ab-5492-407f-94c8-234a3b3baa92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce4fcc-53c9-47c9-9a96-b937c737de28",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.512589Z",
     "iopub.status.idle": "2025-12-24T15:15:18.513587Z",
     "shell.execute_reply": "2025-12-24T15:15:18.512589Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.512589Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.sort_values(by='R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeb20d-7992-4109-8800-b20fea685653",
   "metadata": {},
   "source": [
    "After doing multiple trial and error runs on my LSTM neural network I learned the following:\n",
    "1. Do not try to predict on log returns; it is better to try to predict on standardized data.\n",
    "2. Make sure your lookback window matches your hidden layer.\n",
    "3. You should increase your epochs in order to make the model fit the data better, but if you increase it by too much, then it could increase the loss in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd734f22-5e7f-457a-bb23-f6bb08949a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee20c6-aa1f-4a09-8535-d77ce82af506",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T15:15:18.513587Z",
     "iopub.status.idle": "2025-12-24T15:15:18.514586Z",
     "shell.execute_reply": "2025-12-24T15:15:18.514586Z",
     "shell.execute_reply.started": "2025-12-24T15:15:18.514586Z"
    }
   },
   "outputs": [],
   "source": [
    "for firm in standardized_df.columns:\n",
    "    # get true and predicted values\n",
    "    y_true = standardized_df[firm][-len(all_predictions[firm]):]\n",
    "    y_pred = all_predictions[firm]\n",
    "\n",
    "    # plot the values\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.plot(y_true, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(\n",
    "        f'True vs Predicted Values for {firm}'\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ed74b-92b9-459a-b366-d49b258a9fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T23:56:24.280293Z",
     "iopub.status.busy": "2025-12-19T23:56:24.275153Z",
     "iopub.status.idle": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply": "2025-12-19T23:56:24.284036Z",
     "shell.execute_reply.started": "2025-12-19T23:56:24.280293Z"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c7621-6109-430b-9bb5-87e11aa23b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
